{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2e40a02f89247c2bba1b3ef056ccfd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85d2c5eaa570468aa796538219675018",
              "IPY_MODEL_c254ced514f846a2b57bad66db6e0bdb",
              "IPY_MODEL_482636d1be53413690942ded7cf5056b"
            ],
            "layout": "IPY_MODEL_c487e36e9619448d97b12de090a624d4"
          }
        },
        "85d2c5eaa570468aa796538219675018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b021a220016d4453a691f79b7f88f1c7",
            "placeholder": "​",
            "style": "IPY_MODEL_f7c8df86c3504fb8966466c9576c280d",
            "value": "100%"
          }
        },
        "c254ced514f846a2b57bad66db6e0bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85610478595483f8f8a986562146866",
            "max": 38,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_134b9911dca643ff94073a0a05fe210c",
            "value": 38
          }
        },
        "482636d1be53413690942ded7cf5056b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dcb37014a9d4be2b2b5368808c56c0d",
            "placeholder": "​",
            "style": "IPY_MODEL_846b54805a9644adbff2574db27c9af7",
            "value": " 38/38 [00:07&lt;00:00,  4.84ba/s]"
          }
        },
        "c487e36e9619448d97b12de090a624d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b021a220016d4453a691f79b7f88f1c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7c8df86c3504fb8966466c9576c280d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a85610478595483f8f8a986562146866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "134b9911dca643ff94073a0a05fe210c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dcb37014a9d4be2b2b5368808c56c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "846b54805a9644adbff2574db27c9af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2530b0452f5b44249b11acd7c9d31c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b470285d131497886bc186fa535166a",
              "IPY_MODEL_84df44847d7743769e110b416f3d4ada",
              "IPY_MODEL_b140c32180484f07a39c7a6595ef3bd7"
            ],
            "layout": "IPY_MODEL_17eeeadf0f1448b9b9f0a8bfc6379559"
          }
        },
        "0b470285d131497886bc186fa535166a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e86a2140b0494892949dcc30ff29ffd6",
            "placeholder": "​",
            "style": "IPY_MODEL_42711cef1b524edabdc2a21d5adedfcf",
            "value": "100%"
          }
        },
        "84df44847d7743769e110b416f3d4ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f112f78b594c7ca2a8159d9523bd05",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b12c0e1761e47ef97d328a283a07abd",
            "value": 5
          }
        },
        "b140c32180484f07a39c7a6595ef3bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ae9712c08b94a66a7864a925a2f1639",
            "placeholder": "​",
            "style": "IPY_MODEL_d063d31449d243ab8b9a8870a323f06d",
            "value": " 5/5 [00:00&lt;00:00,  4.76ba/s]"
          }
        },
        "17eeeadf0f1448b9b9f0a8bfc6379559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e86a2140b0494892949dcc30ff29ffd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42711cef1b524edabdc2a21d5adedfcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2f112f78b594c7ca2a8159d9523bd05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b12c0e1761e47ef97d328a283a07abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ae9712c08b94a66a7864a925a2f1639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d063d31449d243ab8b9a8870a323f06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46e8d4314d5242a991914ce9ca82fd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ba9c14a34c6498e9a182736f5d00e03",
              "IPY_MODEL_75af201e8d4640de84e5283c345eacb9",
              "IPY_MODEL_16601462855e492b94579ca5fe07cd74"
            ],
            "layout": "IPY_MODEL_0146aec4ec494dce99cfa60815309a94"
          }
        },
        "7ba9c14a34c6498e9a182736f5d00e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd02bafad96c41f5a29e01fb9c51452a",
            "placeholder": "​",
            "style": "IPY_MODEL_936716714b7a497296456196bbc2cca2",
            "value": "Downloading: 100%"
          }
        },
        "75af201e8d4640de84e5283c345eacb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_003b07dfe9ac429090250fccd1f04f41",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3146e10d4c8496484fb26d48e66511f",
            "value": 501200538
          }
        },
        "16601462855e492b94579ca5fe07cd74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61d6f23bd922457abe26f65b19933dc1",
            "placeholder": "​",
            "style": "IPY_MODEL_2cc3dab3094142ffa86ca05d2b84111a",
            "value": " 478M/478M [00:13&lt;00:00, 41.4MB/s]"
          }
        },
        "0146aec4ec494dce99cfa60815309a94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd02bafad96c41f5a29e01fb9c51452a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936716714b7a497296456196bbc2cca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "003b07dfe9ac429090250fccd1f04f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3146e10d4c8496484fb26d48e66511f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61d6f23bd922457abe26f65b19933dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc3dab3094142ffa86ca05d2b84111a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f78f94a55a234a91b00e467512fe1c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de98b1644aec475695d6ed0a0258fafc",
              "IPY_MODEL_c92df2e63ba046cab807837d94781330",
              "IPY_MODEL_8107bb5bd34f4b35961d07da2144f368"
            ],
            "layout": "IPY_MODEL_50b03a488d864795bfd4458a90034c22"
          }
        },
        "de98b1644aec475695d6ed0a0258fafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35d0bf9527b843908253c6ffe8df5ade",
            "placeholder": "​",
            "style": "IPY_MODEL_835faa11c3d343e3a18856ed9525571e",
            "value": "100%"
          }
        },
        "c92df2e63ba046cab807837d94781330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d05de1b874b4f83869bf4647f9bb0c0",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58e6a7b209554ef39d0c71d75072598f",
            "value": 6
          }
        },
        "8107bb5bd34f4b35961d07da2144f368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e63154e8910645f1849b089a9fbe7df1",
            "placeholder": "​",
            "style": "IPY_MODEL_84dcb90919474252a602f531f7119771",
            "value": " 6/6 [00:31&lt;00:00,  5.17s/ba]"
          }
        },
        "50b03a488d864795bfd4458a90034c22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d0bf9527b843908253c6ffe8df5ade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835faa11c3d343e3a18856ed9525571e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d05de1b874b4f83869bf4647f9bb0c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e6a7b209554ef39d0c71d75072598f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e63154e8910645f1849b089a9fbe7df1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84dcb90919474252a602f531f7119771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3OS1p_azj9w",
        "outputId": "f6c34606-c89f-490b-870c-026bca7b1bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instal Libraries"
      ],
      "metadata": {
        "id": "iCS-2otFffM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==1.0.2\n",
        "!pip install transformers\n",
        "!pip install rouge_score\n",
        "import datasets\n",
        "import transformers\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import EncoderDecoderModel\n",
        "from transformers import TrainingArguments,Seq2SeqTrainer,AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments,DataCollatorForSeq2Seq\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8N2fOdn44tc",
        "outputId": "0c5e6c80-27f6-45b3-bddf-f58a60448547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==1.0.2 in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (4.63.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "rm: cannot remove 'seq2seq_trainer.py': No such file or directory\n",
            "--2022-04-13 00:55:19--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-04-13 00:55:19 ERROR 404: Not Found.\n",
            "\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.21.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.0.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "woHGN8YyfjNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "path = \"/content/drive/MyDrive/NN_Project/amazon_review_dataset_processed.csv\"\n",
        "df = pd.read_csv(path)\n",
        "amazon = Dataset.from_pandas(df)\n",
        "amazon.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q2eq61u3tlp",
        "outputId": "f1aa39f0-7bdc-4022-bd5e-c8d7cea49241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11848, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NotTest_Test = amazon.train_test_split(test_size=0.1,seed=42)\n",
        "NotTest = NotTest_Test[\"train\"]\n",
        "test = NotTest_Test[\"test\"]\n",
        "\n",
        "Train_Val = NotTest.train_test_split(test_size=0.1,seed=42)\n",
        "train = Train_Val[\"train\"]\n",
        "val = Train_Val[\"test\"]\n"
      ],
      "metadata": {
        "id": "4WGq21_k5bSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "tokenizer.bos_token = tokenizer.cls_token\n",
        "tokenizer.eos_token = tokenizer.sep_token\n",
        "batch_size=200\n",
        "encoder_max_length=50\n",
        "decoder_max_length=10\n",
        "\n",
        "def process_data_to_model_inputs(batch):\n",
        "  # tokenize the inputs and labels\n",
        "  inputs = tokenizer(batch[\"reviewText\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "  outputs = tokenizer(batch[\"summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "  batch[\"input_ids\"] = inputs.input_ids\n",
        "  batch[\"attention_mask\"] = inputs.attention_mask\n",
        "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
        "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "  batch[\"labels\"] = outputs.input_ids.copy()\n",
        "  # ignoring PAD token as Roberta shifts labels\n",
        "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "\n",
        "  return batch\n",
        "\n",
        "#preparing training and validation data\n",
        "train = train.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"reviewText\", \"summary\"]\n",
        ")\n",
        "train.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "\n",
        "val = val.map(\n",
        "    process_data_to_model_inputs, \n",
        "    batched=True, \n",
        "    batch_size=batch_size, \n",
        "    remove_columns=[\"reviewText\", \"summary\"]\n",
        ")\n",
        "val.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f2e40a02f89247c2bba1b3ef056ccfd3",
            "85d2c5eaa570468aa796538219675018",
            "c254ced514f846a2b57bad66db6e0bdb",
            "482636d1be53413690942ded7cf5056b",
            "c487e36e9619448d97b12de090a624d4",
            "b021a220016d4453a691f79b7f88f1c7",
            "f7c8df86c3504fb8966466c9576c280d",
            "a85610478595483f8f8a986562146866",
            "134b9911dca643ff94073a0a05fe210c",
            "2dcb37014a9d4be2b2b5368808c56c0d",
            "846b54805a9644adbff2574db27c9af7",
            "2530b0452f5b44249b11acd7c9d31c3a",
            "0b470285d131497886bc186fa535166a",
            "84df44847d7743769e110b416f3d4ada",
            "b140c32180484f07a39c7a6595ef3bd7",
            "17eeeadf0f1448b9b9f0a8bfc6379559",
            "e86a2140b0494892949dcc30ff29ffd6",
            "42711cef1b524edabdc2a21d5adedfcf",
            "d2f112f78b594c7ca2a8159d9523bd05",
            "2b12c0e1761e47ef97d328a283a07abd",
            "5ae9712c08b94a66a7864a925a2f1639",
            "d063d31449d243ab8b9a8870a323f06d"
          ]
        },
        "id": "hPpbe9317QT8",
        "outputId": "ea8f30a2-46bc-4fb6-8815-79dd0c4ec3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/38 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2e40a02f89247c2bba1b3ef056ccfd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2530b0452f5b44249b11acd7c9d31c3a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Encoder-Decoder & Training\n",
        "\n"
      ],
      "metadata": {
        "id": "Kos8f1yTgFA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EncoderDecoderModel\n",
        "\n",
        "roberta_shared = EncoderDecoderModel.from_encoder_decoder_pretrained(\"roberta-base\", \"roberta-base\", tie_encoder_decoder=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "46e8d4314d5242a991914ce9ca82fd5b",
            "7ba9c14a34c6498e9a182736f5d00e03",
            "75af201e8d4640de84e5283c345eacb9",
            "16601462855e492b94579ca5fe07cd74",
            "0146aec4ec494dce99cfa60815309a94",
            "dd02bafad96c41f5a29e01fb9c51452a",
            "936716714b7a497296456196bbc2cca2",
            "003b07dfe9ac429090250fccd1f04f41",
            "e3146e10d4c8496484fb26d48e66511f",
            "61d6f23bd922457abe26f65b19933dc1",
            "2cc3dab3094142ffa86ca05d2b84111a"
          ]
        },
        "id": "GuVbzeLn7s16",
        "outputId": "a619ee87-9384-4479-cb5d-a4004867cc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46e8d4314d5242a991914ce9ca82fd5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.self.query.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set tokens\n",
        "roberta_shared.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
        "roberta_shared.config.eos_token_id = tokenizer.eos_token_id                             \n",
        "roberta_shared.config.max_length = 50\n",
        "roberta_shared.config.early_stopping = True\n",
        "roberta_shared.config.no_repeat_ngram_size = 3\n",
        "roberta_shared.config.length_penalty = 2.0\n",
        "roberta_shared.config.num_beams = 2\n",
        "roberta_shared.config.vocab_size = roberta_shared.config.encoder.vocab_size\n",
        "roberta_shared.config.pad_token_id =1\n",
        "\n"
      ],
      "metadata": {
        "id": "E8Z97oPy79ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = datasets.load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    #removing unecessary tokens\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    result = metric.compute(predictions=pred_str, references=label_str,  use_stemmer=True)\n",
        "    result = {key: round(value.mid.fmeasure * 100, 1) for key, value in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "2oqn1FmMY6kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./\",\n",
        " \n",
        "    predict_with_generate=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "   \n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True\n",
        "\n",
        ")\n",
        "\n",
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=roberta_shared,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=val,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a1qAhlun_81d",
        "outputId": "b4b8bc02-5994-4dad-a7b3-33a8a5534488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using amp half precision backend\n",
            "The following columns in the training set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 9596\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 28788\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28788' max='28788' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28788/28788 1:40:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge2 Precision</th>\n",
              "      <th>Rouge2 Recall</th>\n",
              "      <th>Rouge2 Fmeasure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.483300</td>\n",
              "      <td>5.614880</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.322500</td>\n",
              "      <td>5.427926</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.823100</td>\n",
              "      <td>5.027303</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./checkpoint-500\n",
            "Configuration saved in ./checkpoint-500/config.json\n",
            "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-8500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-1000\n",
            "Configuration saved in ./checkpoint-1000/config.json\n",
            "Model weights saved in ./checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-9000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-1500\n",
            "Configuration saved in ./checkpoint-1500/config.json\n",
            "Model weights saved in ./checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-9500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-2000\n",
            "Configuration saved in ./checkpoint-2000/config.json\n",
            "Model weights saved in ./checkpoint-2000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-2500\n",
            "Configuration saved in ./checkpoint-2500/config.json\n",
            "Model weights saved in ./checkpoint-2500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-3000\n",
            "Configuration saved in ./checkpoint-3000/config.json\n",
            "Model weights saved in ./checkpoint-3000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-1500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-3500\n",
            "Configuration saved in ./checkpoint-3500/config.json\n",
            "Model weights saved in ./checkpoint-3500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-4000\n",
            "Configuration saved in ./checkpoint-4000/config.json\n",
            "Model weights saved in ./checkpoint-4000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-2500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-4500\n",
            "Configuration saved in ./checkpoint-4500/config.json\n",
            "Model weights saved in ./checkpoint-4500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-5000\n",
            "Configuration saved in ./checkpoint-5000/config.json\n",
            "Model weights saved in ./checkpoint-5000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-3500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-5500\n",
            "Configuration saved in ./checkpoint-5500/config.json\n",
            "Model weights saved in ./checkpoint-5500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-6000\n",
            "Configuration saved in ./checkpoint-6000/config.json\n",
            "Model weights saved in ./checkpoint-6000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-4500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-6500\n",
            "Configuration saved in ./checkpoint-6500/config.json\n",
            "Model weights saved in ./checkpoint-6500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-5000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-7000\n",
            "Configuration saved in ./checkpoint-7000/config.json\n",
            "Model weights saved in ./checkpoint-7000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-5500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-7500\n",
            "Configuration saved in ./checkpoint-7500/config.json\n",
            "Model weights saved in ./checkpoint-7500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-6000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-8000\n",
            "Configuration saved in ./checkpoint-8000/config.json\n",
            "Model weights saved in ./checkpoint-8000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-6500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-8500\n",
            "Configuration saved in ./checkpoint-8500/config.json\n",
            "Model weights saved in ./checkpoint-8500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-7000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-9000\n",
            "Configuration saved in ./checkpoint-9000/config.json\n",
            "Model weights saved in ./checkpoint-9000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-7500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-9500\n",
            "Configuration saved in ./checkpoint-9500/config.json\n",
            "Model weights saved in ./checkpoint-9500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-8000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1067\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./checkpoint-10000\n",
            "Configuration saved in ./checkpoint-10000/config.json\n",
            "Model weights saved in ./checkpoint-10000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-8500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-10500\n",
            "Configuration saved in ./checkpoint-10500/config.json\n",
            "Model weights saved in ./checkpoint-10500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-9000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-11000\n",
            "Configuration saved in ./checkpoint-11000/config.json\n",
            "Model weights saved in ./checkpoint-11000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-9500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-11500\n",
            "Configuration saved in ./checkpoint-11500/config.json\n",
            "Model weights saved in ./checkpoint-11500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-10000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-12000\n",
            "Configuration saved in ./checkpoint-12000/config.json\n",
            "Model weights saved in ./checkpoint-12000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-10500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-12500\n",
            "Configuration saved in ./checkpoint-12500/config.json\n",
            "Model weights saved in ./checkpoint-12500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-11000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-13000\n",
            "Configuration saved in ./checkpoint-13000/config.json\n",
            "Model weights saved in ./checkpoint-13000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-11500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-13500\n",
            "Configuration saved in ./checkpoint-13500/config.json\n",
            "Model weights saved in ./checkpoint-13500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-12000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-14000\n",
            "Configuration saved in ./checkpoint-14000/config.json\n",
            "Model weights saved in ./checkpoint-14000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-12500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-14500\n",
            "Configuration saved in ./checkpoint-14500/config.json\n",
            "Model weights saved in ./checkpoint-14500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-13000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-15000\n",
            "Configuration saved in ./checkpoint-15000/config.json\n",
            "Model weights saved in ./checkpoint-15000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-13500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-15500\n",
            "Configuration saved in ./checkpoint-15500/config.json\n",
            "Model weights saved in ./checkpoint-15500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-14000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-16000\n",
            "Configuration saved in ./checkpoint-16000/config.json\n",
            "Model weights saved in ./checkpoint-16000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-14500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-16500\n",
            "Configuration saved in ./checkpoint-16500/config.json\n",
            "Model weights saved in ./checkpoint-16500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-15000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-17000\n",
            "Configuration saved in ./checkpoint-17000/config.json\n",
            "Model weights saved in ./checkpoint-17000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-15500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-17500\n",
            "Configuration saved in ./checkpoint-17500/config.json\n",
            "Model weights saved in ./checkpoint-17500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-16000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-18000\n",
            "Configuration saved in ./checkpoint-18000/config.json\n",
            "Model weights saved in ./checkpoint-18000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-16500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-18500\n",
            "Configuration saved in ./checkpoint-18500/config.json\n",
            "Model weights saved in ./checkpoint-18500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-17000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-19000\n",
            "Configuration saved in ./checkpoint-19000/config.json\n",
            "Model weights saved in ./checkpoint-19000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-17500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1067\n",
            "  Batch size = 1\n",
            "Saving model checkpoint to ./checkpoint-19500\n",
            "Configuration saved in ./checkpoint-19500/config.json\n",
            "Model weights saved in ./checkpoint-19500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-18000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-20000\n",
            "Configuration saved in ./checkpoint-20000/config.json\n",
            "Model weights saved in ./checkpoint-20000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-18500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-20500\n",
            "Configuration saved in ./checkpoint-20500/config.json\n",
            "Model weights saved in ./checkpoint-20500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-19000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-21000\n",
            "Configuration saved in ./checkpoint-21000/config.json\n",
            "Model weights saved in ./checkpoint-21000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-19500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-21500\n",
            "Configuration saved in ./checkpoint-21500/config.json\n",
            "Model weights saved in ./checkpoint-21500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-20000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-22000\n",
            "Configuration saved in ./checkpoint-22000/config.json\n",
            "Model weights saved in ./checkpoint-22000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-20500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-22500\n",
            "Configuration saved in ./checkpoint-22500/config.json\n",
            "Model weights saved in ./checkpoint-22500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-21000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-23000\n",
            "Configuration saved in ./checkpoint-23000/config.json\n",
            "Model weights saved in ./checkpoint-23000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-21500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-23500\n",
            "Configuration saved in ./checkpoint-23500/config.json\n",
            "Model weights saved in ./checkpoint-23500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-22000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-24000\n",
            "Configuration saved in ./checkpoint-24000/config.json\n",
            "Model weights saved in ./checkpoint-24000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-22500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-24500\n",
            "Configuration saved in ./checkpoint-24500/config.json\n",
            "Model weights saved in ./checkpoint-24500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-23000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-25000\n",
            "Configuration saved in ./checkpoint-25000/config.json\n",
            "Model weights saved in ./checkpoint-25000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-23500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-25500\n",
            "Configuration saved in ./checkpoint-25500/config.json\n",
            "Model weights saved in ./checkpoint-25500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-24000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-26000\n",
            "Configuration saved in ./checkpoint-26000/config.json\n",
            "Model weights saved in ./checkpoint-26000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-24500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-26500\n",
            "Configuration saved in ./checkpoint-26500/config.json\n",
            "Model weights saved in ./checkpoint-26500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-25000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-27000\n",
            "Configuration saved in ./checkpoint-27000/config.json\n",
            "Model weights saved in ./checkpoint-27000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-25500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-27500\n",
            "Configuration saved in ./checkpoint-27500/config.json\n",
            "Model weights saved in ./checkpoint-27500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-26000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-28000\n",
            "Configuration saved in ./checkpoint-28000/config.json\n",
            "Model weights saved in ./checkpoint-28000/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-26500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "Saving model checkpoint to ./checkpoint-28500\n",
            "Configuration saved in ./checkpoint-28500/config.json\n",
            "Model weights saved in ./checkpoint-28500/pytorch_model.bin\n",
            "Deleting older checkpoint [checkpoint-27000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1067\n",
            "  Batch size = 1\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=28788, training_loss=5.283505315596451, metrics={'train_runtime': 6042.2912, 'train_samples_per_second': 4.764, 'train_steps_per_second': 4.764, 'total_flos': 792170908896960.0, 'train_loss': 5.283505315596451, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/finetunedModelRoberta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6eFMwkfBH_u",
        "outputId": "678f2f9c-f1b1-4517-ada9-1ec9bfa6c1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/finetunedModelRoberta\n",
            "Configuration saved in /content/drive/MyDrive/finetunedModelRoberta/config.json\n",
            "Model weights saved in /content/drive/MyDrive/finetunedModelRoberta/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned = AutoModelForSeq2SeqLM.from_pretrained(\"/content/drive/MyDrive/finetunedModelRoberta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqT04n-WB-4m",
        "outputId": "49ca80a8-42c4-40e2-91c7-9e54c517d56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Summaries"
      ],
      "metadata": {
        "id": "xYKANvw4gTMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/finetunedModelRoberta\")\n",
        "model.to(\"cuda\")\n",
        "batch_size = 200\n",
        "\n",
        "def generate_summary(batch):\n",
        "    inputs = tokenizer(batch[\"reviewText\"], padding=\"max_length\", truncation=True, max_length=40, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask,num_beams=2,\n",
        "                                  repetition_penalty=3.0, \n",
        "                                  length_penalty=2.0, \n",
        "                                  num_return_sequences = 1)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    batch[\"pred\"] = output_str\n",
        "    return batch\n",
        "\n",
        "results = test.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"reviewText\"])\n",
        "\n",
        "pred_str = results[\"pred\"]\n",
        "\n",
        "label_str = results[\"summary\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "f78f94a55a234a91b00e467512fe1c08",
            "de98b1644aec475695d6ed0a0258fafc",
            "c92df2e63ba046cab807837d94781330",
            "8107bb5bd34f4b35961d07da2144f368",
            "50b03a488d864795bfd4458a90034c22",
            "35d0bf9527b843908253c6ffe8df5ade",
            "835faa11c3d343e3a18856ed9525571e",
            "6d05de1b874b4f83869bf4647f9bb0c0",
            "58e6a7b209554ef39d0c71d75072598f",
            "e63154e8910645f1849b089a9fbe7df1",
            "84dcb90919474252a602f531f7119771"
          ]
        },
        "id": "Q_uCfEizCBhY",
        "outputId": "a9157bee-ed3f-4b58-8480-1111cb633fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f78f94a55a234a91b00e467512fe1c08"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "asWDiXxsgWri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#200 batch\n",
        "print(\"ROUGE 1 SCORE: \",rouge.compute(predictions=pred_str, references=label_str, use_stemmer=True, rouge_types=[\"rouge1\"])[\"rouge1\"])\n",
        "print(\"ROUGE 2 SCORE: \",rouge.compute(predictions=pred_str, references=label_str, use_stemmer=True, rouge_types=[\"rouge2\"])[\"rouge2\"])\n",
        "print(\"ROUGE L SCORE: \",rouge.compute(predictions=pred_str, references=label_str, use_stemmer=True, rouge_types=[\"rougeL\"])[\"rougeL\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvfP6EWuDnwu",
        "outputId": "b8adade0-b111-444f-efdd-a1460b11f794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE 1 SCORE:  AggregateScore(low=Score(precision=0.00036568213783403654, recall=0.0007650642255072638, fmeasure=0.0004779787720327444), mid=Score(precision=0.0007501172058134083, recall=0.0015466003757143, fmeasure=0.000961528403275501), high=Score(precision=0.001228551336146273, recall=0.0024675048931378047, fmeasure=0.001556136961392555))\n",
            "ROUGE 2 SCORE:  AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))\n",
            "ROUGE L SCORE:  AggregateScore(low=Score(precision=0.0003279418659165497, recall=0.0006975389238047467, fmeasure=0.0004343102872607443), mid=Score(precision=0.0007032348804500701, recall=0.00139978165294621, fmeasure=0.0008809752418524263), high=Score(precision=0.0011533052039381156, recall=0.002275793475160564, fmeasure=0.0014249856992190684))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reference: https://anubhav20057.medium.com/step-by-step-guide-abstractive-text-summarization-using-roberta-e93978234a90"
      ],
      "metadata": {
        "id": "T2Uabu5LZA3z"
      }
    }
  ]
}